# LazyA AI Features Documentation

## Overview

LazyA integrates AI as a first-class citizen of the language, not as an afterthought. This document describes all AI-native features.

## Semantic Operators

### Similarity Operator `~=`

Compares **conceptual similarity** rather than exact values using cosine similarity of embeddings.

```lazy
if user_input ~= "insulto" {
    print("Toxic content detected")
}
```

**How it works**:
1. Both operands are converted to vector embeddings using TensorFlow Lite
2. Cosine similarity is calculated between vectors
3. Returns `true` if similarity > 0.7 (default threshold)

**Custom threshold**:
```lazy
if word ~= "hello" threshold 0.85 {
    print("Very similar to 'hello'")
}
```

**Use cases**:
- Content moderation without explicit word lists
- Multilingual comparison
- Fuzzy matching
- Semantic search

### Implication Operator `~>`

Checks if one concept semantically implies another.

```lazy
if "perro" ~> "animal" {
    print("Correct: dogs are animals")
}
```

**Implementation**:
- Uses directional similarity in embedding space
- Considers hierarchical relationships
- Asymmetric (A ~> B doesn't mean B ~> A)

## Context Functions

Functions where the implementation is **generated by AI** based on a docstring.

### Syntax

```lazy
@context
func function_name(params) -> return_type {
    """
    Detailed description of what the function should do.
    Be specific about inputs, outputs, and behavior.
    """
}
```

### Example

```lazy
@context
func analyze_sentiment(text: string) -> float {
    """
    Analyzes the sentiment of the given text.
    Returns -1.0 (very negative) to 1.0 (very positive).
    Considers context, sarcasm, and cultural nuances.
    """
}

// Usage - no manual implementation needed!
sentiment = analyze_sentiment("This movie was terrible!")
print(sentiment)  // Output: -0.8
```

### Cold Start vs Warm Start

#### Cold Start
- First time running the program
- AI generates implementation from docstring
- Code is compiled and cached
- Slower (may take seconds)

```bash
lazya program.lazy --cold-start
```

#### Warm Start (Default)
- Uses cached implementation
- No AI generation needed
- Fast execution (milliseconds)

```bash
lazya program.lazy  # uses cache
```

### Implementation Process

1. **Parse docstring**: Extract intent, parameters, return type
2. **Generate code**: Call LLM to write implementation
3. **Validate**: Type-check and test generated code
4. **Cache**: Store compiled binary with signature hash
5. **Reuse**: Load from cache on subsequent runs

### Best Practices

**Good docstrings**:
```lazy
@context
func extract_emails(text: string) -> [string] {
    """
    Extracts all email addresses from the input text.
    Returns a list of valid email strings.
    Uses regex pattern matching for RFC 5322 compliance.
    """
}
```

**Bad docstrings** (too vague):
```lazy
@context
func do_stuff(data: any) -> any {
    """
    Does something with data.
    """
}
```

## LLM Integration

### Native LLM Calls

Call language models directly from LazyA code.

#### Local Models

```lazy
response = llm.complete(
    "Explain recursion",
    model="local/llama3"
)
```

Supported local backends:
- `local/llama3`: Meta's Llama 3 via llama.cpp
- `local/mistral`: Mistral 7B
- `local/phi`: Microsoft Phi-2

#### Cloud Models

```lazy
response = llm.complete(
    "Write a poem",
    model="cloud/gpt-4",
    temperature=0.9,
    max_tokens=100
)
```

Supported cloud providers:
- `cloud/gpt-4`: OpenAI GPT-4
- `cloud/claude`: Anthropic Claude
- `cloud/gemini`: Google Gemini

### Structured Generation

Force LLM to return structured data:

```lazy
result = llm.generate(
    prompt="Extract info from: " + text,
    format="json",
    schema={
        "name": "string",
        "age": "int",
        "email": "string"
    }
)

print(result.name)  // Type-safe access
```

### Configuration

Set API keys in `~/.lazya/config.toml`:

```toml
[llm.openai]
api_key = "sk-..."

[llm.anthropic]
api_key = "sk-ant-..."

[llm.local]
model_path = "/path/to/models/"
```

## Embeddings

### Direct Embedding Access

```lazy
vec = embed("artificial intelligence")
// Returns: [0.234, -0.123, 0.456, ...]
```

### Similarity Calculation

```lazy
vec1 = embed("cat")
vec2 = embed("dog")
similarity = cosine_similarity(vec1, vec2)
print(similarity)  // 0.82 (quite similar)
```

### Use Cases

- Semantic search
- Clustering
- Recommendation systems
- Anomaly detection

## Caching Strategy

### Multi-Level Cache

1. **Memory cache**: Frequently used embeddings
2. **Disk cache**: Generated code binaries
3. **Remote cache**: Shared team cache (optional)

### Cache Invalidation

Cache is invalidated when:
- Docstring changes
- Function signature changes
- LazyA compiler version changes

### Manual Cache Control

```lazy
@context @cached(ttl=3600)  // Cache for 1 hour
func expensive_operation(data: string) -> string {
    """
    ...
    """
}

@context @cached(false)  // Never cache
func get_current_time() -> string {
    """
    Returns the current timestamp.
    """
}
```

## Performance Considerations

### Embedding Performance

- **First call**: ~10-50ms (model loading + inference)
- **Subsequent calls**: ~1-2ms (cached in memory)
- **Model size**: ~80MB (Universal Sentence Encoder)

### LLM Performance

#### Local
- **First token**: 50-200ms
- **Tokens/sec**: 20-50 (depends on hardware)
- **Memory**: 4-8GB RAM for 7B models

#### Cloud
- **Latency**: 100-500ms (network + generation)
- **Rate limits**: Provider-dependent
- **Cost**: ~$0.001-0.03 per 1K tokens

### Context Function Performance

- **Cold start**: 5-30 seconds (LLM generation + compilation)
- **Warm start**: <10ms (load from cache)
- **Cache size**: ~100KB - 10MB per function

## Model Management

### Downloading Models

```bash
# Download embedding model
lazya models download sentence-encoder

# Download local LLM
lazya models download llama3-7b
```

### Custom Models

```lazy
// Use your own embedding model
vec = embed("text", model="custom/my_encoder.tflite")

// Use fine-tuned LLM
response = llm.complete(
    prompt,
    model="local/my_finetuned_model"
)
```

## Safety & Privacy

### Data Privacy

- **Local models**: All data stays on your machine
- **Cloud models**: Data sent to provider (read their privacy policy)
- **Embeddings**: Computed locally by default

### Content Filtering

```lazy
@context
func is_safe_content(text: string) -> bool {
    """
    Returns true if content is safe for work.
    Checks for violence, explicit content, hate speech.
    """
}

if !is_safe_content(user_message) {
    print("Content blocked")
}
```

### Rate Limiting

Built-in rate limiting for cloud LLM calls:

```toml
[llm.limits]
requests_per_minute = 60
tokens_per_day = 100000
```

## Future Features (Roadmap)

- [ ] Multi-modal embeddings (images, audio)
- [ ] Streaming LLM responses
- [ ] Fine-tuning interface
- [ ] Distributed cache
- [ ] GPU acceleration
- [ ] Quantized models
- [ ] Hybrid local/cloud fallback

## Debugging AI Features

### Verbose Mode

```bash
lazya program.lazy --ai-verbose
```

Shows:
- Embedding computation time
- LLM token usage
- Cache hits/misses
- Generated code for context functions

### Dry Run

```bash
lazya program.lazy --ai-dry-run
```

Shows what would be generated without actually calling LLMs.

## Examples

See `examples/` directory:
- `02_ai_operators.lazy`: Semantic operators
- `03_context_function.lazy`: AI-generated functions
- `04_llm_call.lazy`: LLM integration
